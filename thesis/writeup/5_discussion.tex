\chapter{Discussion}\label{ch:discussion}

The reader should now be comfortable with sequential model based optimization, especially of the EGO variety. We have seen how to minimize an expensive blackbox function by focusing our efforts on our ability to model that function, choosing sample points to maximize the predictive power of our models. Specifically, I have focused on the maximum expected improvement criterion for sample point selection, which both exploits available knowledge of the response surface, and explores unknown regions of the optimization domain. As shown in the demonstration at the end of Chapter \ref{ch:smbo}, this strategy explores unknown regions only enough to be confident that they do not contain a the global optimum---this way, we do not waste expensive evaluations of the blackbox function making precise models of suboptimal regions. Thus we generate predictive models that are very accurate to the response surface only in certain regions of the optimization domain, precisely those regions that were at some point compelling candidates for the global optimum; it is a waste of blackbox evaluations to sample extensively elsewhere in the domain. SMBO is frugal; each evaluation of the expensive blackbox function is rigorously chosen to best further the overall optimization goal. There is perhaps some sense of meta-optimization afoot: in the task of globally optimizing $f$, we carefully choose samples to optimize our ability to locate $f$'s optimum.

As the discussion so far has centered largely on the EGO algorithm, the next section briefly describes some other members of the SMBO class. I will also compare the SMBO paradigm to another interesting method of global optimization: genetic algorithms. There is an interesting analogy between the two strategies, and I make a case for thinking of SMBO as a clever improvement on GAs. This will lead to a discussion of further research I'd like to see done regarding SMBO, especially as it relates to simulated evolution and artificial life.

\section{SMBO beyond EGO}\label{sec:next_smbo}

Recall the three-step characterization of SMBO, most recently illustrated in Figure \ref{fig:smbo_loop_II}. Step one relies only upon the function being optimized; it is independent of the particulars of the SMBO algorithm at hand. The simplest generalization of the EGO algorithm swaps out the second, modelling step of the SMBO process for a method other than DACE that nonetheless produces a prediction surface with expected variance, $(\hat{f},\err)$.

As discussed in Section \ref{sec:max_imp}, it is straightforward to use a prediction model $(\hat{f},\err)$ to construct the expected improvement function over $f$'s domain, a highly desirable criterion for sample selection. So regardless of whether the DACE model occupied step two of the SMBO loop, step three---the sampling step---works the same as long as we have both the prediction $\hat{f}$ and $\err$. It is fair to characterize SMBO processes such as these as ``EGO without DACE''.

Having the option of using models beyond DACE drastically increases the scope of SMBO. For example, it is fairly straightforward to generalize DACE to make a predictor function for noisy or probabilistic, rather than deterministic functions.\footnote{\cite{jones_efficient_1998}}. One can also make models that allow for discrete, rather than continuous, input dimensions---this is especially useful when optimizing algorithm parameters as in \cite{hutter_sequential_2011}.

Though DACE is of course a good model with its own merits, especially the BLUP\footnote{Best Linear Unbiased Predictor} property, there are many other fish in the sea. There is a huge variety of algorithms that can generate a prediction surface from a set of sample data: decision trees, neural networks, and good old fashioned linear regression, to name a few.
In many of these cases, however, a predicted variance function $\err$ is less immediately accessible than in the DACE model. For example, a neural network produces a prediction model from a set of sample points via supervised learning, but it is not obvious how to quantify the accuracy of that network's prediction on a novel sample. This is problematic, because without variance $\err$, we cannot use the maximum expected improvement criterion to select samples in the third step of the SMBO loop.

There are at least two ways to approach this problem. One is to only attempt SMBO using models for which rigorously defined error functions are accessible. The other is to use a method other than expected improvement maximization to select new samples. In some of its applications, the SMBO-centric company ProtoLife takes this tack when they use neural networks as predictive models. While I extol the theoretical virtues of the exploration-exploitation balance accomplished by maximizing expected improvement, they devised two separate sampling methods, one that samples to explore unsampled regions, and another to exploit promising regions within the current model. To arrive at one sampling method that balances these two, each new sample is chosen by one or the other sampling method according to some fixed probability (e.g., ``30\% chance to choose to explore, 70\% chance to choose to exploit''). By using heuristic alternatives to maximum expected improvement, that nonetheless both explore and exploit the domain space, SMBO still works well enough for at least ProtoLife's industrial biochemical applications.

It is notable that once we generalize our notion of SMBO to allow for heuristic sampling methods, all three steps of the SMBO loop are modular, and the defining feature of SMBO involves no particular mathematics. Now, we see that SMBO at its most general is perhaps little more than what is shown in the popular visualization of the SMBO loop I have reproduced in Figures \ref{fig:smbo_cycle} and \ref{smbo_loop_II}: a black box, a modeller, and a sampler, all working together, to lift themselves up by eachother's bootstraps and generate models with greater and greater predictive power.


\section{A Rival to Genetic Algorithms}

There is a conceptual case to be made that sequential model-based optimization is just a souped-up version of the popular combinatorial optimization strategy known as Genetic Algorithms, or GAs. A two-sentence summary of GAs for our purposes is this: from a population of candidate optima (sample points), the best performing samples are used to choose new sample points for function evaluation by a method akin to the way new organisms are born from their parents' genetic material. The idea is then to ``evolve'' the global optimum\footnote{or simply a good-but-not optimal solution, as GAs are often used with this more relaxed goal.} The fitness function which ranks parents in a GAs sample population corresponds to the blackbox function in SMBO.

GAs are of course conceptually interesting for their mimicry of biological evolution, which as we know from biology, has discovered and/or created very good solutions to seemingly hard problems, such as ``how to fly''. However, there are a number of dimensions along which SMBO seems like an obviously better solution to general combinatorial optimization problems than GAs.

For one, GAs generally only balance exploration and exploitation by combining random mutation with differential reproductive fitness (the fact that especially `fit' sample points influence more future samples than less fit points do), which seems fairly heavy-handed and random compared to the calculated care of selecting new samples by maximizing the expected improvement function. An SMBO algorithm, unlike to a GA, has the ability to look at a candidate sample that hasn't been evaluated yet, and say, ``Even though I've not seen it before, I predict the value will be so poor that it's not even worth evaluating the objective function.'' On the other hand, random mutations in GAs are generally uniformly distributed, as if to say that everywhere in possible-genome-space is equally worthy of exploration.

Another benefit of SMBO over classical GAs is that, in SMBO, information about every point sampled so far is integrated to select the next sample to evaluate, whereas in GAs usually only one or two previous sample points are considered in the introduction of a new candidate solution. Because getting stuck in local optima is such a pervasive problem in global optimization [cite] and GAs in particular [cite] suffer as a result of the inbreeding-like conditions that arise as a result of elite selection, the globally-aware sampling criteria of expected improvement is an appealing alternative.

\section{Looking Forward}

The main unresolved goal of this thesis is the package \texttt{smbo}, which produced the beautiful plots we have seen of 1D optimization using the EGO algorithm, but admittedly does not do much else very well. The extant code-base is somewhat meticulously constructed, though, and modular with the intent of being easily expanded upon. In making \texttt{smbo} open-source and available on the Python Package Index, I have given it a life beyond this assignment, and I for one plan on being an active contributor to the nascent \texttt{smbo} community.

The next logical step for \texttt{smbo} is to perform optimization of blackbox functions which are significantly more complicated than the 1D toy problem in Chapter \ref{ch:smbo}. The work by \cite{hutter_automated_2009} on hyperparameter optimization and the algorithm selection problem is especially interesting, and I would like to see \texttt{smbo} applied to the task of optimizing the performance of a commercial SAT solver in a specific problem domain---this is perhaps the `hello world' of hyperparameter optimization, and good performance on this task would bode well for the use of \texttt{smbo} to optimize real-world blackbox functions.

In the spirit of the discussion regarding Genetic Algorithms, I would also like to see what evolving populations would look like if new individuals were born by the SMBO sample-selection process rather than traditional natural selection and genetic combination plus mutation. Imagine a population of creatures, where the genome of every newborn is a intelligently, carefully-crafted synthesis of all previous genomes, with a few hand-picked mutations: each child designed to best further the survival of the species at large. A goal of mine is to make this a reality in a population of virtual creatures, to use \texttt{smbo} to `hyperevolve' classic artificial life models such as Reynold's Boids or Avida, and see if the populations' evolutionary trajectory is qualitatively different from those of populations under natural selection.

Depending on the results of that kind of research, it would be interesting to investigate one of the largest problems in Artificial Life with SMBO. In ALife, much has been said about `open-ended evolution,' the empirical observation that biological life has no apparent upper-limit on complexity, and evolution regularly creates totally novel forms. This phenomena has never been replicated in artificially evolving systems, and there is much discussion as to why this has been the case. I am inspired by the fact that in performing SMBO it seems that we optimize our own ability to optimize, and this is why it is easy to think of SMBO as a bootstrapping process. I suspect that an artificial life system with some similar complexity-bootstrapping process, where we generate novel artificial organisms so as to optimize our ability to generate yet more novel organisms, could reveal some insight into the nature of open-ended evolution.



