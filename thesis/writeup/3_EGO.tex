\chapter{Case study: the EGO Algorithm}\label{ch:ego}

The EGO algorithm is named for the paper in which it was presented, the informatively titled, ``Efficient Global Optimization of Expensive Blackbox Functions'' \citep{jones_efficient_1998}. I treat the EGO algorithm as the quintessential SMBO--it was the first, and it makes simple and intuitive assumptions about both the objective function and the best method to model it. [Kinda a lame explanation, but there has got to be more justification for treating EGO so centrally. One compelling thread is that the DACE model is provably the 'best linear unbiased predictor' (good wiki page on that)]. 

\section{The DACE Predictor}\label{sec:dace}
The model that is sequentially fit in the EGO algorithm is known as the DACE predictor. Like EGO, the DACE acronym comes from a somewhat generally-titled paper, in this case, ``Design and Analysis of Computer Experiments'' \citep{sacks_design_1989}. [Might want to elaborate later. For now, here's the bare info:] The assumptions that the DACE predictor makes are these (following closely Jones et al's presentation of DACE \cite{jones_efficient_1998}):

First, we assume what is called a \emph{stochastic process model} [get cites/explanation from Jones p.456], i.e., that,
\begin{equation} \label{eq:stoch_proc}
y(\X^{(i)}) = \mu + \epsilon(\X^{(i)}) \ \ \ \ \ \text{for } i \in (1,2,...,n).
\end{equation}
As is common in statistics, $\mu$ represents the mean of the process. Note that the above equation appears simpler than even linear regression, as it has no functional component. The DACE model, and stochastic processes in general, instead contains its predictive power in the `error terms' $\epsilon(\X^{(i)})$. These terms are assumed to be distributed normally,

\begin{equation} \label{eq:dace_err}
\epsilon(\X^{(i)}) = \text{Normal}(0,\sigma^2)\ \ \ \ \ \text{for } i \in (1,2,...,n),
\end{equation}

for a process-wide $\sigma^2$. Despite the normal distribution, the $\epsilon(\X^{(i)})$ are very much \emph{not} independent of each other: it is in a complex error-correlation structure that the DACE model encodes the contours of its response surface. Specifically,

\begin{equation} \label{eq:dace_corr}
\text{Corr}\left(\epsilon(\X^{(i)}),\epsilon(\X^{(i)})\right)\ \  = \ \ 
	\text{Exp}
		\left [ 
			-\sum_{h=1}^{k} 
				\theta_h \left | \X^{(i)}_h - \X^{(j)}_h \right | ^{p_h}
		\right ]\ \  = \ \ 
	\prod_{h=1}^{k}
		\text{Exp}
			\left [
				-\theta_h \left | \X^{(i)}_h - \X^{(j)}_h \right | ^{p_h}
			\right ],
\end{equation}

where the free parameters $\{(\theta_i,p_i)$ for $i \in (1,2,...,k)$ determine the shape of the DACE predictor. Note that these three constraints completely describe the DACE model: we assume that we are modeling a stochastic process with mean $\mu$, standard deviation $\sigma^2$, and inter-sample correlation described by Eq. \ref{eq:dace_corr}.

%write
Here I should discuss what error correlation means with some pretty pictures like in Jones p.459.

Much more can (and should, and will) be said about the shape of the predictor implied by this correlation equation, but for now it suffices to say that it encodes the heuristic, ``points near each other in input-space should have nearby function values'', with a concept of nearness along each input dimension that is gaussian in shape, with magnitude and falloff-steepness determined by $\theta$ and $p$.

\subsection{Selection of $\{(\theta_i,p_i)\}$}\label{sec:max_lik}
To fit a DACE predictor to sample data, the $k+2$ free parameters $\mu, \sigma^2,$ and $\{(\theta_i,p_i)\}$ for $i \in (1,2,...,k)$ are set by maximizing the likelihood function which is implied by the prior assumptions \ref{eq:stoch_proc, eq:dace_err, eq:dace_corr}. I will here walk through the derivation of that likelihood function.

Likelihood, I'll remind the reader, is a statistical concept very similar to probability. % get a dank intro stats textbook here.
Imagine a scenario where parameters $\psi$ give rise to some model $f_\psi$. Given some empirical observations $Z$, the concept of likelihood allows us to quantify how well the parameter choice $\psi$ and the generated model $f_\psi$ match our empirical observation---likelihood tests how well a model matches data. This is done quite simply: by defining the likelihood of a set of parameters given an observation, as the probability of that observation, given those parameters:

\begin{equation} \label{eq:def_likelihood}
L(\psi|Z) = Pr(Z|\psi).
\end{equation}

Note that this works intuitively well with our notion of probability: a set of parameters is a good one, i.e. it is likely, if the model that it generates is one wherein our observed data are relatively probable. The best model is that which is most likely, meaning that under no other model would the observed data $Z$ appear more probable.

Here, our observed data is the vector of witnessed function values $\Y$, so finding an equation for the likelihood of our parameters is equivalent to deriving the joint probability distribution of this $n$-vector, given the assumptions laid out in the previous section. I will now derive this joint distribution, using techniques that should be familiar to those schooled in college-level statistics. I'll first derive a simpler distribution, then use the change of variables formula to arrive at the result.

Consider a set of independent, identically-distributed gaussian random variables $\mathbb{Z} = Z_1,...,Z_n$, with mean 0 and standard deviation $\sigma^2$. For the sake of being explicit, the expectation function $f_{Z_i}$ for each $Z_i$ is then,

\begin{equation} \label{eq:f_{Z_i}}
f_{Z_i}=\frac{1}{\sigma\sqrt{2\pi}}e^{-Z_i^2/2\sigma^2}.
\end{equation}

The joint probability distribution of $\mathbb{Z}$ is simply the product of each of its [independent] components:

\begin{align}  \label{eq:F_Z}
f_\mathbb{Z}(\mathbb{Z}) &= \prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}}e^{-Z_i^2/2\sigma^2}tf  \\
			 &= \frac{1}{(2\pi\sigma^2)^{n/2}} {E}xp\left [ -\frac{1}{2\sigma^2}\sum_{i=1}^{n} Z_i^2\right ] \nonumber\\
			 &= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{-\mathbb{Z}^T\mathbb{Z}/2\sigma^2},\nonumber
\end{align}

where the last line uses vector multiplication to denote the sum of the square components of $\mathbb{Z}$.

Now, we use the change of variables formula to derive from this an equation for our actual data. For convenience, I'll consider the probability distribution of the vector of error terms, $\mathbb{\epsilon}$, rather than simply the output vector $\mathbbold{y}$. If $\mathbbold{1}$ denotes the 1-vector, then,

\begin{equation}\label{eq:epsilon}
\epsilon = \mathbbold{y} - \mathbbold{1}\mu.
\end{equation}

I will also consider $\mathbb{R}$, the \emph{correlation matrix} of the errors $\epsilon$. $\mathbb{R}$ is simply the $n\times n$ matrix whose $(i,j)^{th}$ entry is $Corr[\epsilon_i,\epsilon_j]$ as defined in Eq \ref{eq:dace_corr}. By construction, we know that $\mathbb{R}$ is symmetric and positive-definite, which means that a Cholesky decomposition exists CITE\cite{linear_algebra}, i.e., that there exists a lower-triangular matrix $A$ such that,

\begin{equation} \label{eq:cholesky}
\mathbb{R} = A A^T.
\end{equation}

Now, because of [WHY? is it the lower-triangularity of $A$? I don't see how we know that the difference between $\epsilon$ and iid is the matrix A], we know that,
\begin{equation} \label{Y_sub}
\epsilon=A\mathbb{Z},
\end{equation}

where $\mathbb{Z}$ is and independent, identically distributed vector with each component Normal$(0,\sigma^2)$. We will use this fact, along with the change of variables formula and the above-derived probability density function for such a $\mathbb{Z}$ (Eq \ref{eq:F_Z}), to arrive at the likelihood equation. Letting $g$ denote the linear transformation equivalent to left-multiplication by the matrix A, we see that,

\begin{align}
\epsilon &= g(\mathbb{Z}) \nonumber \\
\mathbb{Z} &= g^{-1}(\epsilon) = A^{-1}\epsilon
\label{eq:sub}
\end{align}



Now recall the change of variables formula, which given Eq. \ref{eq:sub} states that,
\begin{align} \label{eq:change_of_vars}
f_\epsilon &= f_\mathbb{Z}\left (g^{-1}(\mathbb{Z})\right)\times \left | g^{-1}(\mathbb{Z}) \right | \\
			 &= f_\mathbb{Z}\left (A^{-1}\epsilon\right) |A^{-1}| \\ 
			 &= f_\mathbb{Z}\left (A^{-1}\epsilon\right) \frac{1}{|A|}
\end{align}

Now plugging in Eq. \ref{eq:F_Z}, the formula for $f_\mathbb{Z}$, 

\begin{align} \label{eq:getting_F_Y}
f_\epsilon &= \frac{1}{(2\pi\sigma^2)^{n/2}|A|} {E}xp\left [ -\frac{1}{2\sigma^2}(A^{-1}\epsilon)^T(A^{-1}\epsilon)\right ].
\end{align}

Now I will do some linear algebra to the matrices in the exponent,

\begin{align}
(A^{-1}\epsilon)^T(A^{-1}\epsilon) &= \epsilon^T(A^{-1})^TA^{-1}\epsilon \nonumber\\
			 						 &= \epsilon^T((A^T)^{-1}A^{-1})\epsilon\nonumber\\
									 &= \epsilon^T(AA^T)^{-1}\epsilon\nonumber\\
									 &= \epsilon^T\mathbb{R}^{-1}\epsilon
\end{align}

Using this result, and also rewriting $|A|$ as $|\mathbb{R}|^{1/2}$ (by Eq. \ref{eq:cholesky} and the fact that the determinant of a product is the product of the determinants), we get the clean formulation,

\begin{equation} \label{eq:likelihood1}
\frac{1}
  {(2\pi\sigma^2)^{n/2}|\mb{R}|^{1/2}}\ 
Exp \left 
  [ -\frac
    {\epsilon^T\mathbb{R}^{-1}\epsilon}
    {2\sigma^2} 
\right ],
\end{equation}

which was written by Jones et al as,

\begin{equation} \label{eq:likelihood}
\frac{1}
  {(2\pi\sigma^2)^{n/2}|\mb{R}|^{1/2}}\ 
Exp \left 
  [ -\frac
    {(\mb{y}-\mb{1}\mu)^T\mb{R}^{-1}(\mb{y}-\mb{1}\mu)}
    {2\sigma^2} 
\right ].
\end{equation}

If we fix the $\{(\theta_i,p_i)\}$, the above equation can be analytically maximized in $\mu$ and $\sigma^2$ \citep{jones_efficient_1998} to get,

\begin{equation} \label{eq:mu_hat}
\hat{\mu}=\frac
	{\mb{1}^T\mb{R}^{-1}\mb{y}}
	{\mb{1}^T\mb{R}^{-1}\mb{1}},
\end{equation}


and,

\begin{equation} \label{eq:sig_hat}
\hat{\sigma}^2
    = \frac{(\mb{y}-\mb{1}\hat{\mu})^T\mb{R}^{-1}(\mb{y}-\mb{1}\hat{\mu})}{n} 
	= \frac{\epsilon^T\mb{R}^{-1}\epsilon}{n}.
\end{equation}

Note that when $\mb{R}=\mb{I}$, the identity matrix, there is no correlation between each variable. As you would hope, in this case Eq. \ref{eq:mu_hat} and Eq. \ref{eq:sig_hat} reduce to the standard statistical definitions of mean and standard deviation.

By substituting Eq. \ref{eq:mu_hat} and Eq. \ref{eq:sig_hat} into the likelihood equation (Eq \ref{eq:likelihood}), we get the concentrated likelihood equation, which in practice is numerically maximized to fit the DACE parameters $p,\ \theta$:

\begin{equation} \label{eq:conc_likelihood}
\frac{e^{n/2}}
  {(2\pi\hat{\sigma}^2)^{n/2}|\mb{R}|^{1/2}},
\end{equation}
