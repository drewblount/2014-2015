
\documentclass[letterpaper]{report}
\usepackage{graphicx}
\graphicspath{ {images/} }
\newcommand{\X}{\mathbf{x}}
\newcommand{\Y}{\mathbf{y}}
\newcommand{\R}{\mathbf{R}}

%\usepackage{natbib,alifeconf}
\usepackage{natbib,amsmath}
\usepackage[top=1.5in, bottom=1.5in, left=1.25in, right=1.25in]{geometry}

% for the results table
\usepackage{booktabs}
\usepackage{multirow,bigstrut}
\usepackage{tabu}

\usepackage{palatino}
\usepackage{pxfonts}

%\usepackage{amsmath}

%% Macros
\newcommand{\mb}{\mathbf}


\DeclareSymbolFont{bbold}{U}{bbold}{m}{n}
\DeclareSymbolFontAlphabet{\mathbbold}{bbold}

%%



\title{Sequential Model-Based Optimization\\ {\small \textit{-- of --}} \\Expensive Blackbox Functions}
\author{Drew Blount \\
\mbox{}\\
Mathematics Department, Reed College\\
\\
dblount@reed.edu}

\begin{document}
\maketitle


\chapter{The SMBO Paradigm}

Sequential model-based optimization is an accurately named process, because it attempts to optimize objective functions by developing a sequence of models. The broad idea is this: you want to understand some process's global behavior from a few sample points, and you have a limited ability to collect more data--think of additional samples as available, but expensive to gather. This is often the case with processes that are difficult to observe or simulate.

Given a few sample points, you use the available data to develop a model of the process's behavior.  With this model alone, you could perform simple \emph{model}-based optimization, using it to predict the locations of global optima. In \emph{sequential} model-based optimization, however, we use predictive models in a more clever, bootstrapping way: to predict what further data, if collected, would allow us to improve our model---and thus our predictive ability---the most. In other words, each model is used to generate another, better model. Global optima, or at least very good solutions, are then found by recursively improving the predictive ability of a model.

To make this process more tangible, consider the case of gold-mining, which is a surprisingly deep analogy to the kind of data mining which is explored in this thesis. Imagine that you are a gold-miner, and you own a claim to Valley X. You want to understand where in the valley you could find the most gold, where best to start a mine. Your mining company has drilled five exploratory shafts throughout the valley. You make a map of the valley's gold distribution based on these five samples.

Were you, the gold miner, only interested in model-based optimization, you would use this rudimentary map to predict where the most gold in the valley is, and start your mine there. Say, however, that you have enough resources to drill five more exploratory shafts first. The clever miner then asks themself, ``where could I drill the next exploratory shaft, to best learn about where the gold is most concentrated in the valley?" By leveraging what they have learned from the first five data points, the miner finds the region they would like most to learn about. After drilling the sixth hole in this region, the miner can improve their model of the gold distribution yet again, and ask the same question: ``where should I investigate next to find the most gold?'' The goal of this thesis is to automate this decision-making process.

\section{History}

[Just a few paragraphs here, talk first about actual kriging (which was done for gold) and maybe get a fun historical anecdote]

[Talk about the EGO paper \citep{jones_efficient_1998}--Hutter calls it the beginning of SMBO, ``limited to optimizing continuous parameters for noise-free functions (i.e., the performance of deterministic algorithms).'' \cite[p.~509]{hutter_sequential_2011} See citation for further discussion of SMBO history. Talk about the background of EGO authors, how Jones worked for GM and one of the case studies in the original paper involved 3D engine component design.]

Mention ``PDT$^{TM}$,'' ProtoLife's ``predictive design technology,'' which is illustrated by a figure very similar to Fig \ref{fig:smbo_cycle} \cite{protolife_pdt_2013}

Talk about Hutter's recent work, maybe Google talk? Perhaps some words about how the ML community isn't particularly interested right now, and is more interested in accomplishing extremely huge-dimensional mapping problems like vision and translation, which are being accomplished by deep nets.


\section{The SMBO Cycle}

This thesis will explore several algorithms that are classified as sequential model-based optimizers \cite{hutter_sequential_2011, hamadi_autonomous_2012, jones_efficient_1998, rasmussen_gaussian_2006}. %note: get more specific with those cites. should be a handful of specific papers presenting particular SMBO things
They differ in the space of model functions they explore, their assumptions regarding determinism, and their procedural relationship to the objective functions being optimized. % clarification: such as some algorithms selecting one sample point, others more.
Nonetheless, they can all be roughly summarized by a three-part while loop,

\begin{enumerate} \label{smbo_loop}
\item the objective function is evaluated at some set of input points
\item a working model of the objective function (the ``predictor function'') is generated from the evaluated input-output pairs
\item new points are chosen to be evaluated by the objective function, so that the working model's utility in optimizing the objective function will be maximized.
\end{enumerate}

[note: should the above be in pseudocode? It would at least be nice to reference it like a figure.]

The loop is run until results are satisfactory, or the expected improvement from further iterations falls below a user-defined threshold. Because of the circular, bootstrapping nature of the algorithm, it is tempting to illustrate it with a triangle like we see on recycling bins, as shown in Figure \ref{fig:smbo_cycle}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{EGO_cycle}
	\caption{the three iterated stages of the SMBO process}
	\label{fig:smbo_cycle}

\end{figure}

Tho company ProtoLife uses a similar illustration to Fig. \ref{fig:smbo_cycle} to describe their analytical product, which appears to be a powerful research applications of SMBO \cite{protolife_pdt_2013}. The top node in the triangle is determined by each domain-specific application. Like ProtoLife, I am interested in trying several different model modules (the bottom-right node in the figure) to optimize different objective functions (the top-right node).

[Hutter aslo includes in his introduction an image of a predictor function w/ expected improvement overlayed, like in Jones, right here]

\section{Terms, Symbols}

The function which we wish to optimize is the \emph{objective function}. The \emph{predictor} or \emph{model function} is that generated in each iteration of the \emph{SMBO loop} (or \emph{cycle}). 

\emph{Sample points} are those points where we have evaluated the objective function--the points on which we base our model function. Throughout this thesis, I'll use several letters and symbols for specific concepts and indices. So that there is a definitive reference, I have listed them here (make this an official table/figure?).

\begin{description}
  \item[$n$]: the number of sample points
  \item[$k$]: the dimensionality of input space
  \item[$\X^{(i)}$]: the $i$th sample point (a $k$-vector)
  \item[$\X$]: the vector of sample points ($n$ $k$-vectors, an $n\times k$ matrix)
  \item[$y$]: the objective function
  \item[$\hat{y}$]: the predictor function
  \item[$\Y$]: the vector of evaluated outputs, i.e. $\Y_i=y(\X^{(i)})$
\end{description}


\chapter{Case study: the EGO Algorithm}

The EGO algorithm is named for the paper in which it was presented, the informatively titled, ``Efficient Global Optimization of Expensive Blackbox Functions'' \citep{jones_efficient_1998}. I treat the EGO algorithm as the quintessential SMBO--it was the first, and it makes simple and intuitive assumptions about both the objective function and the best method to model it. [Kinda a lame explanation, but there has got to be more justification for treating EGO so centrally]. 

\section{The DACE Predictor}
The model that is sequentially fit in the EGO algorithm is known as the DACE predictor. Like EGO, the DACE acronym comes from a somewhat generally-titled paper, in this case, ``Design and Analysis of Computer Experiments'' \citep{sacks_design_1989}. [Might want to elaborate later. For now, here's the bare info:] The assumptions that the DACE predictor makes are these (following closely Jones et al's presentation of DACE \cite{jones_efficient_1998}):

First, we assume what is called a \emph{stochastic process model} [get cites/explanation from Jones p.456], i.e., that,
\begin{equation} \label{eq:stoch_proc}
y(\X^{(i)}) = \mu + \epsilon(\X^{(i)}) \ \ \ \ \ \text{for } i \in (1,2,...,n).
\end{equation}
As is common in statistics, $\mu$ represents the mean of the process. Note that the above equation appears simpler than even linear regression, as it has no functional component. The DACE model, and stochastic processes in general, instead contains its predictive power in the `error terms' $\epsilon(\X^{(i)})$. These terms are assumed to be distributed normally,

\begin{equation} \label{eq:dace_err}
\epsilon(\X^{(i)}) = \text{Normal}(0,\sigma^2)\ \ \ \ \ \text{for } i \in (1,2,...,n),
\end{equation}

for a process-wide $\sigma^2$. Despite the normal distribution, the $\epsilon(\X^{(i)})$ are very much \emph{not} independent of each other: it is in a complex error-correlation structure that the DACE model encodes the contours of its response surface. Specifically,

\begin{equation} \label{eq:dace_corr}
\text{Corr}\left(\epsilon(\X^{(i)}),\epsilon(\X^{(i)})\right)\ \  = \ \ 
	\text{Exp}
		\left [ 
			-\sum_{h=1}^{k} 
				\theta_h \left | \X^{(i)}_h - \X^{(j)}_h \right | ^{p_h}
		\right ]\ \  = \ \ 
	\prod_{h=1}^{k}
		\text{Exp}
			\left [
				-\theta_h \left | \X^{(i)}_h - \X^{(j)}_h \right | ^{p_h}
			\right ],
\end{equation}

where the free parameters $\{(\theta_i,p_i)$ for $i \in (1,2,...,k)$ determine the shape of the DACE predictor. Note that these three constraints completely describe the DACE model: we assume that we are modeling a stochastic process with mean $\mu$, standard deviation $\sigma^2$, and inter-sample correlation described by Eq. \ref{eq:dace_corr}.

%write
Here I should discuss what error correlation means with some pretty pictures like in Jones p.459.

Much more can (and should, and will) be said about the shape of the predictor implied by this correlation equation, but for now it suffices to say that it encodes the heuristic, ``points near each other in input-space should have nearby function values'', with a concept of nearness along each input dimension that is gaussian in shape, with magnitude and falloff-steepness determined by $\theta$ and $p$.

\subsection{Selection of $\{(\theta_i,p_i)\}$}
To fit a DACE predictor to sample data, the $k+2$ free parameters $\mu, \sigma^2,$ and $\{(\theta_i,p_i)$ for $i \in (1,2,...,k)$ are set by maximizing the likelihood function which is implied by the prior assumptions \ref{eq:stoch_proc, eq:dace_err, eq:dace_corr}. I will here walk through the derivation of that likelihood function.

Likelihood, I'll remind the reader, is a statistical concept very similar to probability. % get a dank intro stats textbook here.
Imagine a scenario where parameters $\psi$ give rise to some model $f_\psi$. Given some empirical observations $Z$, the concept of likelihood allows us to quantify how well the parameter choice $\psi$ and the generated model $f_\psi$ match our empirical observation---likelihood tests how well a model matches data. This is done quite simply: by defining the likelihood of a set of parameters given an observation, as the probability of that observation, given those parameters:

\begin{equation} \label{eq:def_likelihood}
L(\psi|Z) = Pr(Z|\psi).
\end{equation}

Note that this works intuitively well with our notion of probability: a set of parameters is a good one, i.e. it is likely, if the model that it generates is one wherein our observed data are relatively probable. The best model is that which is most likely, meaning that under no other model would the observed data $Z$ appear more probable.

Here, our observed data is the vector of witnessed function values $\Y$, so finding an equation for the likelihood of our parameters is equivalent to deriving the joint probability distribution of this n-vector, given the assumptions laid out in the previous section. I will now derive this joint distribution, using techniques that should be familiar to those schooled in college-level statistics. I'll first derive a simpler distribution, then use the change of variables formula to arrive at the result.

Consider a set of independent, identically-distributed gaussian random variables $\mathbb{Z} = Z_1,...,Z_n$, with mean 0 and standard deviation $\sigma^2$. For the sake of being explicit, the expectation function $f_{Z_i}$ for each $Z_i$ is then,

\begin{equation} \label{eq:f_{Z_i}}
f_{Z_i}=\frac{1}{\sigma\sqrt{2\pi}}e^{Z_i^2/\sigma^2}.
\end{equation}

The joint probability distribution of $\mathbb{Z}$ is simply the product of each of its [independent] components:

\begin{align}  \label{eq:F_Z}
f_\mathbb{Z}(\mathbb{Z}) &= \prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}}e^{Z_i^2/\sigma^2} \\
			 &= \frac{1}{(2\pi\sigma^2)^{n/2}} {E}xp\left [ \frac{1}{\sigma^2}\sum_{i=1}^{n} Z_i^2\right ] \nonumber\\
			 &= \frac{1}{(2\pi\sigma^2)^{n/2}} e^{\mathbb{Z}^T\mathbb{Z}/\sigma^2},\nonumber
\end{align}

where the last line uses vector multiplication to denote the sum of the square components of $\mathbb{Z}$.

Now, we use the change of variables formula to derive from this an equation for our actual data. For convenience, I'll consider the probability distribution of the vector of error terms, $\mathbb{\epsilon}$, rather than simply the output vector $\mathbbold{y}$. If $\mathbbold{1}$ denotes the 1-vector, then,

\begin{equation}\label{eq:epsilon}
\epsilon = \mathbbold{y} - \mathbbold{1}\mu.
\end{equation}

I will also consider $\mathbb{R}$, the \emph{correlation matrix} of the errors $\epsilon$. $\mathbb{R}$ is simply the $n\times n$ matrix whose $(i,j)^{th}$ entry is $Corr[\epsilon_i,\epsilon_j]$ as defined in Eq \ref{eq:dace_corr}. By construction, we know that $\mathbb{R}$ is symmetric and positive-definite, which means that a Cholesky decomposition exists CITE\cite{linear_algebra}, i.e., that there exists a lower-triangular matrix $A$ such that,

\begin{equation} \label{eq:cholesky}
\mathbb{R} = A A^T.
\end{equation}

Now, because of [WHY? is it the lower-triangularity of $A$? I don't see how...], we know that the matrix multiplication of $A$ and $\epsilon$ will produce a vector of mutually independent variables sharing the Normal$(0,\epsilon)$ distribution of each component of $\epsilon$. That is,
\begin{equation} \label{Y_sub}
A\epsilon=\mathbb{Z},
\end{equation}

where $\mathbb{Z}$ is and independent, identically distributed vector with each component Normal$(0,\epsilon)$. We will use this fact, along with the change of variables formula and the above-derived probability density function for such a $\mathbb{Z}$ (Eq \ref{eq:F_Z}), to arrive at the likelihood equation. Letting $g$ denote the linear transformation equivalent to left-multiplication by the matrix A, we see that,

\begin{align} \label{eq:sub}
g(\epsilon) &= \mathbb{Z} \\
\epsilon &= g^{-1}(\mathbb{Z}) = A^{-1}\mathbb{Z}
\label{eq:sub1}
\end{align}



Now recall the change of variables formula, which given Eq. \ref{eq:sub1} states that,
\begin{align} \label{eq:change_of_vars}
f_\epsilon &= f_\mathbb{Z}\left (g^{-1}(\mathbb{Z})\right)\times \left | g^{-1}(\mathbb{Z}) \right | \\
			 &= f_\mathbb{Z}\left (A^{-1}\epsilon\right) |A^{-1}| \\ 
			 &= f_\mathbb{Z}\left (A^{-1}\epsilon\right) \frac{1}{|A|}
\end{align}

Now plugging in Eq. \ref{eq:F_Z}, the formula for $f_\mathbb{Z}$, 

\begin{align} \label{eq:getting_F_Y}
f_\epsilon &= \frac{1}{(2\pi\sigma^2)^{n/2}|A|} {E}xp\left [ \frac{1}{\sigma^2}(A^{-1}\epsilon)^T(A^{-1}\epsilon)\right ].\\
\end{align}

Now I will do some linear algebra to the matrices in the exponent,

\begin{align}
(A^{-1}\epsilon)^T(A^{-1}\epsilon) &= \epsilon^T(A^{-1})^TA^{-1}\epsilon \nonumber\\
			 						 &= \epsilon^T((A^T)^{-1}A^{-1})\epsilon\nonumber\\
									 &= \epsilon^T(AA^T)^{-1}\epsilon\nonumber\\
									 &= \epsilon^T\mathbb{R}^{-1}\epsilon
\end{align}

Also rewriting $|A|$ as $|\mathbb{R}|^{1/2}$, because the determinant of a product is the product of the determinants, and Eq. \ref{eq:cholesky}, we get the clean formulation,

\begin{equation} \label{eq:likelihood1}
\frac{1}
  {(2\pi\sigma^2)^{n/2}|\mb{R}|^{1/2}}\ 
Exp \left 
  [ \frac
    {\epsilon^T\mathbb{R}^{-1}\epsilon}
    {\sigma^2} 
\right ].
\end{equation}

[ASK ALBYN:Why am I missing a factor of $-\frac{1}{2}$ over Jones's result (Eq \ref{eq:likelihood})?]

Recall the definition of correlation, that for random variables $a$ and $b$,

\begin{equation} \label{eq:correlation}
\text{Corr}(a,b) = \frac{E\left[ \left ( a - E(a) \right ) \left ( b - E(b) \right ) \right ]}{\sigma_a \sigma_b},
\end{equation}

where $E$ is the expected-value function. Now considering Eq. \ref{eq:dace_err}, we have a simple equation for the correlation of two errors,

\begin{equation} \label{eq:simp_corr}
\text{Corr}(a,b) = \frac{E\left[\epsilon(\X^{(i)})\epsilon(\X^{(i)})\right]}{\sigma^2}
\end{equation}

From here, you can derive the likelihood equation, 

\begin{equation} \label{eq:likelihood}
\frac{1}
  {(2\pi\sigma^2)^{n/2}|\mb{R}|^{1/2}}\ 
exp \left 
  [ -\frac
    {(\mb{y}-\mb{1}\mu)'\mb{R}^{-1}(\mb{y}-\mb{1}\mu)}
    {2\sigma^2} 
\right ].
\end{equation}

If we fix the $\{(\theta_i,p_i)\}$, the above equation can be analytically maximized in $\mu$ and $\sigma^2$ \citep{jones_efficient_1998} to get,

\begin{equation} \label{eq:mu_hat}
\hat{\mu}=\frac
	{\mb{1}'\mb{R}^{-1}\mb{y}}
	{\mb{1}'\mb{R}^{-1}\mb{1}},
\end{equation}


and,

\begin{equation} \label{eq:sig_hat}
\hat{\sigma}^2=\frac
	{(\mb{y}-\mb{1}\hat{\mu})'\mb{R}^{-1}(\mb{y}-\mb{1}\hat{\mu})}
	{n}.
\end{equation}

Note that when $\mb{R}=\mb{I}$, the identity matrix, there is no correlation between each variable. As you would hope, in this case Eq. \ref{eq:mu_hat} and Eq. \ref{eq:sig_hat} reduce to the standard statistical definitions of mean and standard deviation.


\begin{equation} \label{eq:conc_likelihood}
asdfasdfasd
\end{equation}


\chapter{Implementation}

\section{Visual Examples with EGO}
\subsection{1D}
--I've already made the software for this. Narrate a few examples. Probably a one-page grid of figures showing initial sample points, selection of next points, update of the model, etc.

\subsection{2D}
--Perhaps the Branin function shown in the original Jones paper? the same grid would be good to look at in 2D.

\section{Performance Tests}

--Optimize a SAT solver

--Compare to other optimizers and perhaps SMBOs besides EGO

--Maybe use a neural net model function?? (how do we measure expected improvement--how does Norman?)

--Here is where I'll dump all of my time for the rest of March once I'm done with the example implementations.


\bibliographystyle{apalike}
\bibliography{bib/thesis}


\end{document}



