
The most remarkable fact of nature is that a seemingly mechanistic universe of particles and radiation gave rise to life, complexity, and intelligence. It is shocking that the possibilities of migrating birds, human emotion, and consumer technology were implicit in the primordial universe. To me, this is not remarkable for cosmological nor biological reasons, but algorithmic and information-theoretic.

I wish to understand where organization comes from and how systems can increase their own complexity. I see evolutionary biology, complex systems theory, cognitive science, evolutionary computation, and machine learning as a spectrum of disciplines addressing this issue. While machine learning is hardly concerned with questions of cosmic development, it does seek to replicate exactly the phenomenon I am most interested in: the iterative generation of structure where once was noise.

As a researcher, I intend primarily to study methods for detecting organization and patterns in complex systems, and using such tools, research where these structures come from and why. This has been my goal at the Artificial Life Lab at Reed College, where I've studied the US Patent  record over the past year. Viewing the network of patent citations as an enormous and convoluted family tree, I've used  text-mining techniques to formalize the notion of a patent's `traits'. With  traits and a family tree, the patent record becomes an analog of an evolving biological population, and my research project has been investigating the depth of this analogy. Culture, like biological evolution, has the peculiar habit of creating things that truly are new under the sun, so my research asks, is cultural novelty produced by the same functional process as biological innovation? If so, we have learned something new about culture. If not, then among other things, machine learning researchers should look to cultural evolution to find new paradigms of design-space exploration. The question is too big to answer decisively in the short time I've worked on it, so my work has been largely in developing analytic methodologies for citation-linked text corpora. In doing so, I built an 80 GB database, learned two programming languages, and how to hammer out a MapReduce in a minute. 

Simultaneously to researching biological, artificial, and cultural evolution at the ALife Lab, I've been engaged in a research project on neural networks and artificial chemistry at Teuscher.:Lab at Portland State University. With graduate student Peter Banda, I've been working for a year and a half to design and simulate the first learning-capable neural net implemented in a chemical reaction network. We recently achieved our goal, and created the first chemical system capable of learning the exclusive-OR logic function, and I wrote a paper that was submitted to IEEE Trans. Neural Netw. in Dec. 2014. Designing computational systems in novel media and architecture, to me, embodies the notion of a computational universe, where anything can store and process information under the right circumstances. I am lucky to have explored such an interesting concept while gaining practical experience, such as enterprise-level development, exploring design-space with evolutionary algorithms, and co-authoring a technical paper presenting our in-house chemical simulation environment.

On top of my classes and research with the above two labs, I have pursued a completely independent research project on empirically identifying adaptations in evolving populations. This work epitomizes my interest in complexity-generating systems, and my desire to taxonomize them and quantify their study. The project grew out of a paper I wrote for professor Mark Bedau's Philosophy of Biology class. At Prof. Bedau's encouragement, I ran some proof-of-concept measurements in an artificial life environment I built from the ground-up, and developed my findings into an extended abstract, which was presented at the Artificial Life '14 conference. I was later contacted by the conference committee and asked to develop the project into a full paper, to be included in a special journal issue presenting the important results of the conference. This extended version is currently under review.

Research excites me, but the most satisfying thing I have done as an undergraduate has been tutoring the Intro Computing and Algorithms courses at Reed. As a tutor, I have held bi-weekly office hours for one or the other class since September of 2013. I also grade papers, and helped the professor design new assignments and code projects for an updated Algorithms curriculum. This past week, I helped a pair of students implement a Markov model for random text generation, who had never written a line of code in September. These students came to me each week this semester, and witnessing their growth and joy at learning has reaffirmed my interest in computer science, and my faith in educational institutions. A major draw of graduate study is the opportunity to further my teaching career---to get better at sharing interesting ideas with others, and help students discover and understand the concepts that most fascinate me.

I've been a busy undergrad. After getting a nuclear reactor operator's license as a freshman, I realized that I was more interested in computing than in physics. Since then, I've taken every opportunity to learn about computer science at my small college without a CS department. I've worked with friends to do quantitative finance with cryptocurrencies; given three public, hour-long lectures; and created and installed an interactive computer art piece as part of a weekend-long festival. I've become the most prolific computing TA at Reed. I have published two papers, and two more are under review. I truly feel that I am thriving, and am doing now what I want to do for many years: learning, researching, programming, publishing, and teaching. To make that into a career, I need a PhD. 



